"""
XGBoost Model - Quantum Trader Pro
ModÃ¨le de gradient boosting pour prÃ©diction directionnelle
"""

import xgboost as xgb
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple
from datetime import datetime
import joblib
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from utils.logger import setup_logger
from utils.safety import safe_dataframe_access, safe_iloc
from utils.validators import safe_division

class XGBoostModel:
    """
    ModÃ¨le XGBoost pour prÃ©diction de direction du marchÃ©:
    - Classification binaire (UP/DOWN)
    - Feature importance
    - Hyperparameter tuning
    - Model persistence
    """
    
    def __init__(self, config: Dict):
        """
        Initialise le modÃ¨le XGBoost
        
        Args:
            config: Configuration complÃ¨te du bot
        """
        self.config = config
        self.logger = setup_logger('XGBoostModel')
        
        # Configuration XGBoost
        xgb_config = config.get('ml', {}).get('models', {}).get('xgboost', {})
        
        self.n_estimators = xgb_config.get('n_estimators', 200)
        self.max_depth = xgb_config.get('max_depth', 6)
        self.learning_rate = xgb_config.get('learning_rate', 0.1)
        self.objective = xgb_config.get('objective', 'binary:logistic')
        
        # Model
        self.model = None
        self.feature_names = []
        self.feature_importance = {}
        self.training_metrics = {}
        
        # Paths
        self.model_dir = Path('ml_models/saved_models')
        self.model_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger.info("âœ… XGBoost Model initialisÃ©")
    
    def train(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        validation_split: float = 0.2,
        verbose: bool = True
    ) -> Dict:
        """
        EntraÃ®ne le modÃ¨le XGBoost
        
        Args:
            X: Features DataFrame
            y: Target Series
            validation_split: Proportion pour validation
            verbose: Afficher progression
        
        Returns:
            Dict avec mÃ©triques d'entraÃ®nement
        """
        
        self.logger.info(f"ðŸš€ DÃ©but entraÃ®nement XGBoost ({len(X)} samples)")
        
        # Split train/validation
        X_train, X_val, y_train, y_val = train_test_split(
            X, y,
            test_size=validation_split,
            shuffle=False  # Garder ordre temporel
        )
        
        self.logger.info(f"ðŸ“Š Train: {len(X_train)} | Validation: {len(X_val)}")
        
        # Sauvegarder feature names
        self.feature_names = list(X.columns)
        
        # ParamÃ¨tres du modÃ¨le
        params = {
            'n_estimators': self.n_estimators,
            'max_depth': self.max_depth,
            'learning_rate': self.learning_rate,
            'objective': self.objective,
            'eval_metric': 'logloss',
            'random_state': 42,
            'n_jobs': -1,
            'tree_method': 'hist'
        }
        
        # CrÃ©er et entraÃ®ner modÃ¨le
        self.model = xgb.XGBClassifier(**params)
        
        # EntraÃ®nement avec early stopping
        eval_set = [(X_train, y_train), (X_val, y_val)]
        
        self.model.fit(
            X_train, y_train,
            eval_set=eval_set,
            verbose=verbose
        )
        
        # Ã‰valuation
        metrics = self._evaluate(X_train, y_train, X_val, y_val)
        
        # Feature importance
        self._calculate_feature_importance()
        
        # Sauvegarder mÃ©triques
        self.training_metrics = metrics
        self.training_metrics['timestamp'] = datetime.now().isoformat()
        
        self.logger.info(f"âœ… EntraÃ®nement terminÃ©:")
        self.logger.info(f"   - Accuracy (val): {metrics['val_accuracy']:.4f}")
        self.logger.info(f"   - F1 Score (val): {metrics['val_f1']:.4f}")
        self.logger.info(f"   - ROC AUC (val): {metrics['val_roc_auc']:.4f}")
        
        return metrics
    
    def _evaluate(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_val: pd.DataFrame,
        y_val: pd.Series
    ) -> Dict:
        """Ã‰value le modÃ¨le sur train et validation"""
        
        # PrÃ©dictions
        y_train_pred = self.model.predict(X_train)
        y_val_pred = self.model.predict(X_val)
        
        y_train_proba = self.model.predict_proba(X_train)[:, 1]
        y_val_proba = self.model.predict_proba(X_val)[:, 1]
        
        # MÃ©triques
        metrics = {
            # Train
            'train_accuracy': accuracy_score(y_train, y_train_pred),
            'train_precision': precision_score(y_train, y_train_pred),
            'train_recall': recall_score(y_train, y_train_pred),
            'train_f1': f1_score(y_train, y_train_pred),
            'train_roc_auc': roc_auc_score(y_train, y_train_proba),
            
            # Validation
            'val_accuracy': accuracy_score(y_val, y_val_pred),
            'val_precision': precision_score(y_val, y_val_pred),
            'val_recall': recall_score(y_val, y_val_pred),
            'val_f1': f1_score(y_val, y_val_pred),
            'val_roc_auc': roc_auc_score(y_val, y_val_proba)
        }
        
        # Check overfitting
        accuracy_diff = metrics['train_accuracy'] - metrics['val_accuracy']
        if accuracy_diff > 0.1:
            self.logger.warning(
                f"âš ï¸ Possible overfitting: Train acc - Val acc = {accuracy_diff:.4f}"
            )
        
        return metrics
    
    def _calculate_feature_importance(self):
        """Calcule l'importance des features"""
        
        if self.model is None:
            return
        
        # Get importance scores
        importance_scores = self.model.feature_importances_
        
        # CrÃ©er dict
        self.feature_importance = {
            feature: float(score)
            for feature, score in zip(self.feature_names, importance_scores)
        }
        
        # Trier par importance
        self.feature_importance = dict(
            sorted(self.feature_importance.items(), key=lambda x: x[1], reverse=True)
        )
        
        # Log top 10
        self.logger.info("ðŸ“Š Top 10 features importantes:")
        for i, (feature, score) in enumerate(list(self.feature_importance.items())[:10], 1):
            self.logger.info(f"   {i}. {feature}: {score:.4f}")
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """
        PrÃ©diction binaire (0 ou 1)
        
        Args:
            X: Features DataFrame
        
        Returns:
            Array de prÃ©dictions (0=DOWN, 1=UP)
        """
        
        if self.model is None:
            raise ValueError("ModÃ¨le non entraÃ®nÃ©")
        
        # VÃ©rifier features
        if list(X.columns) != self.feature_names:
            self.logger.warning("âš ï¸ Features diffÃ©rentes, rÃ©ordonnancement")
            X = X[self.feature_names]
        
        predictions = self.model.predict(X)
        
        return predictions
    
    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """
        PrÃ©diction de probabilitÃ©s
        
        Args:
            X: Features DataFrame
        
        Returns:
            Array de probabilitÃ©s [P(DOWN), P(UP)]
        """
        
        if self.model is None:
            raise ValueError("ModÃ¨le non entraÃ®nÃ©")
        
        # VÃ©rifier features
        if list(X.columns) != self.feature_names:
            X = X[self.feature_names]
        
        probabilities = self.model.predict_proba(X)
        
        return probabilities
    
    def get_confidence(self, X: pd.DataFrame) -> np.ndarray:
        """
        Retourne la confidence de la prÃ©diction (0-1)
        
        Args:
            X: Features DataFrame
        
        Returns:
            Array de confidence scores
        """
        
        proba = self.predict_proba(X)
        
        # Confidence = max probability
        confidence = np.max(proba, axis=1)
        
        return confidence
    
    def get_signal_with_confidence(self, X: pd.DataFrame) -> Tuple[int, float]:
        """
        Retourne signal et confidence pour la derniÃ¨re observation

        Args:
            X: Features DataFrame (derniÃ¨re ligne sera utilisÃ©e)

        Returns:
            (signal, confidence) oÃ¹ signal = 1 (UP) ou 0 (DOWN)
        """

        # Validation sÃ©curitÃ©
        if not safe_dataframe_access(X, "xgboost_predict"):
            self.logger.warning("âš ï¸ DataFrame invalide pour prÃ©diction")
            return 0, 0.0

        if len(X) == 0:
            return 0, 0.0

        try:
            # Prendre derniÃ¨re ligne (safe_iloc retourne une Series, on veut un DataFrame)
            X_last = X.iloc[[-1]]

            # PrÃ©diction
            signal = self.predict(X_last)[0]
            proba = self.predict_proba(X_last)[0]

            # Confidence = probabilitÃ© de la classe prÃ©dite
            confidence = proba[signal]

            return int(signal), float(confidence)
        except Exception as e:
            self.logger.error(f"âŒ Erreur prÃ©diction XGBoost: {e}")
            return 0, 0.0
    
    def save(self, filename: Optional[str] = None) -> str:
        """
        Sauvegarde le modÃ¨le
        
        Args:
            filename: Nom du fichier (optionnel)
        
        Returns:
            Chemin du fichier sauvegardÃ©
        """
        
        if self.model is None:
            raise ValueError("Aucun modÃ¨le Ã  sauvegarder")
        
        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'xgboost_{timestamp}.pkl'
        
        filepath = self.model_dir / filename
        
        # Sauvegarder tout
        model_data = {
            'model': self.model,
            'feature_names': self.feature_names,
            'feature_importance': self.feature_importance,
            'training_metrics': self.training_metrics,
            'config': {
                'n_estimators': self.n_estimators,
                'max_depth': self.max_depth,
                'learning_rate': self.learning_rate
            }
        }
        
        joblib.dump(model_data, filepath)
        
        self.logger.info(f"ðŸ’¾ ModÃ¨le sauvegardÃ©: {filepath}")
        
        return str(filepath)
    
    def load(self, filepath: str):
        """
        Charge un modÃ¨le sauvegardÃ©
        
        Args:
            filepath: Chemin vers le fichier
        """
        
        self.logger.info(f"ðŸ“‚ Chargement modÃ¨le: {filepath}")
        
        model_data = joblib.load(filepath)
        
        self.model = model_data['model']
        self.feature_names = model_data['feature_names']
        self.feature_importance = model_data.get('feature_importance', {})
        self.training_metrics = model_data.get('training_metrics', {})
        
        # Restore config
        config = model_data.get('config', {})
        self.n_estimators = config.get('n_estimators', self.n_estimators)
        self.max_depth = config.get('max_depth', self.max_depth)
        self.learning_rate = config.get('learning_rate', self.learning_rate)
        
        self.logger.info("âœ… ModÃ¨le chargÃ© avec succÃ¨s")
        
        if self.training_metrics:
            self.logger.info(
                f"   - Accuracy (val): {self.training_metrics.get('val_accuracy', 0):.4f}"
            )
    
    def optimize_hyperparameters(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        n_trials: int = 50
    ) -> Dict:
        """
        Optimise les hyperparamÃ¨tres avec Optuna
        
        Args:
            X: Features DataFrame
            y: Target Series
            n_trials: Nombre d'essais
        
        Returns:
            Meilleurs paramÃ¨tres trouvÃ©s
        """
        
        try:
            import optuna
        except ImportError:
            self.logger.error("âŒ Optuna non installÃ©: pip install optuna")
            return {}
        
        self.logger.info(f"ðŸ” Optimisation hyperparamÃ¨tres ({n_trials} trials)")
        
        # Split
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, shuffle=False
        )
        
        def objective(trial):
            """Fonction objectif pour Optuna"""
            
            params = {
                'n_estimators': trial.suggest_int('n_estimators', 50, 300),
                'max_depth': trial.suggest_int('max_depth', 3, 10),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),
                'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
                'objective': 'binary:logistic',
                'eval_metric': 'logloss',
                'random_state': 42,
                'n_jobs': -1
            }
            
            model = xgb.XGBClassifier(**params)
            model.fit(X_train, y_train, verbose=False)
            
            y_pred = model.predict(X_val)
            accuracy = accuracy_score(y_val, y_pred)
            
            return accuracy
        
        # Optimisation
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)
        
        best_params = study.best_params
        best_score = study.best_value
        
        self.logger.info(f"âœ… Meilleur score: {best_score:.4f}")
        self.logger.info(f"   ParamÃ¨tres: {best_params}")
        
        # Update config
        self.n_estimators = best_params['n_estimators']
        self.max_depth = best_params['max_depth']
        self.learning_rate = best_params['learning_rate']
        
        return best_params
    
    def get_metrics(self) -> Dict:
        """Retourne les mÃ©triques d'entraÃ®nement"""
        return self.training_metrics.copy()
    
    def get_feature_importance(self, top_n: int = 20) -> Dict:
        """
        Retourne les N features les plus importantes
        
        Args:
            top_n: Nombre de features Ã  retourner
        
        Returns:
            Dict {feature: importance}
        """
        
        if not self.feature_importance:
            return {}
        
        return dict(list(self.feature_importance.items())[:top_n])
